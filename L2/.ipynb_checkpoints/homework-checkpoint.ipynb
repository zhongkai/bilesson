{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1=[[-0.90556482 -0.52841469 -1.62539764  1.89868037  0.91518978 -1.14937478\n",
      "   1.26083088 -0.13171818 -1.69460765 -0.94446493]\n",
      " [-2.92849337 -0.22392885 -0.84831375  0.60805109 -0.08196751  0.46772138\n",
      "   1.09211848  1.67919404 -0.52206655 -0.15978436]\n",
      " [ 0.96038173 -1.17717605  0.76027287  0.27559132  0.26973158  0.13612026\n",
      "   1.7009631  -0.09975877 -1.98730356 -0.2532974 ]\n",
      " [-0.23267643 -1.2209884   0.35508944 -0.61030931  0.93117041  0.03219868\n",
      "  -0.23105087 -0.64046523  0.06940279  0.3846556 ]\n",
      " [ 0.14389543  1.53043823 -0.28462754  1.58677904  1.00417697 -0.53228671\n",
      "  -1.18470999 -1.07353839 -0.67474049 -1.09076168]\n",
      " [ 1.08341897 -1.01542846  0.99400463 -0.59976559 -1.53594652 -0.09898312\n",
      "   0.99992281  0.41936636  0.61026303  0.58379718]\n",
      " [ 0.47296411 -0.03079623 -0.192605    0.19156619  0.02403946 -0.06040372\n",
      "  -0.09012625  2.05993309 -1.05341918 -2.07772376]\n",
      " [-0.18976566 -1.89729086  1.21227932 -0.24843589 -0.86827528 -2.15549996\n",
      "  -0.35891989 -0.44405075 -0.18718351 -0.20104399]\n",
      " [-0.18335198 -1.28201326 -1.34019701  1.04598382  1.39674095  1.8176645\n",
      "   1.16646911  0.44135094 -0.59532101  0.4706242 ]\n",
      " [ 0.09752702  0.10262501 -1.21282544 -0.8299152   0.94778992 -0.73391682\n",
      "   0.4267762   0.72287089  0.09509576 -0.1630574 ]\n",
      " [-1.5617777   0.65367267 -1.48172924 -1.18440427  3.39115637 -0.39968039\n",
      "   0.46310956 -1.37552127  1.67028406  0.16207451]\n",
      " [ 0.40440138 -1.94464206  0.58146882 -0.75902105  0.78185738  0.7660567\n",
      "   0.81683452 -0.4780164   2.1577159  -0.39094512]\n",
      " [ 0.30674841  0.38045685  0.97351246  1.35798536 -0.66238172 -2.36599672\n",
      "  -0.56887821 -1.0667492   0.10502298  0.34922427]] \n",
      " w2=[[ 1.48442503]\n",
      " [ 1.67974272]\n",
      " [ 2.68945115]\n",
      " [-1.38473157]\n",
      " [ 1.27664055]\n",
      " [ 2.74769141]\n",
      " [ 1.14485072]\n",
      " [ 1.43877732]\n",
      " [ 2.51887962]\n",
      " [ 0.34239292]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    使用numpy实现Boston房价预测\n",
    "    Step1 数据加载，来源sklearn中的load_boston\n",
    "    Step2 数据规范化，将X 采用正态分布规范化\n",
    "    Step3 初始化网络\n",
    "    Step4 定义激活函数，损失函数，学习率 epoch\n",
    "    Step5 循环执行：前向传播，计算损失函数，反向传播，参数更新\n",
    "    Step6 输出训练好的model参数，即w1, w2, b1, b2\n",
    "\"\"\" \n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle, resample\n",
    "\n",
    "# 数据加载\n",
    "data = load_boston()\n",
    "X_ = data['data']\n",
    "y = data['target']\n",
    "# 将y转化为矩阵的形式\n",
    "y = y.reshape(y.shape[0],1)\n",
    "\n",
    "# 数据规范化\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
    "\n",
    "\"\"\"\n",
    "    初始化网络参数\n",
    "    定义隐藏层维度，w1,b1,w2,b2\n",
    "\"\"\" \n",
    "n_features = X_.shape[1]\n",
    "n_hidden = 10\n",
    "w1 = np.random.randn(n_features, n_hidden)\n",
    "b1 = np.zeros(n_hidden)\n",
    "w2 = np.random.randn(n_hidden, 1)\n",
    "b2 = np.zeros(1)\n",
    "\n",
    "# relu函数\n",
    "def Relu(x):\n",
    "    return np.where(x < 0, 0, x)\n",
    "\n",
    "\n",
    "# 设置学习率\n",
    "learning_rate = 1e-6\n",
    "\n",
    "# 定义损失函数\n",
    "def MSE_loss(y, y_hat):\n",
    "    return np.mean(np.square(y - y_hat))\n",
    "\n",
    "# 定义线性回归函数\n",
    "def Linear(X, W1, b1):\n",
    "    return X.dot(W1) + b1\n",
    "\n",
    "# 5000次迭代\n",
    "for _ in range(5000):\n",
    "    # 前向传播，计算预测值y (Linear->Relu->Linear)\n",
    "    l1 = Linear(X_, w1, b1)\n",
    "    s1 = Relu(l1)\n",
    "    y_hat = Linear(s1, w2, b2)\n",
    "\n",
    "\n",
    "    # 计算损失函数, 并输出每次epoch的loss\n",
    "    loss = MSE_loss(y, y_hat)\n",
    "    \n",
    "#     print(loss)\n",
    "\n",
    "    # 反向传播，基于loss 计算w1和w2的梯度\n",
    "    grad_y_hat = 2 * (y_hat - y)\n",
    "    grad_w2 = s1.T.dot(grad_y_hat)\n",
    "    grad_relu = grad_y_hat.dot(w2.T)\n",
    "    grad_relu[l1<0] = 0\n",
    "    grad_w1 = X_.T.dot(grad_relu)\n",
    "\n",
    "\n",
    "    # 更新权重, 对w1, w2, b1, b2进行更新\n",
    "    w1 = w1 - learning_rate * grad_w1\n",
    "    w2 = w2 - learning_rate * grad_w2\n",
    "\n",
    "\n",
    "# 得到最终的w1, w2\n",
    "print('w1={} \\n w2={}'.format(w1, w2))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Thinking1:为什么我们需要在神经网络中使用非线性激活函数\n",
    "    \n",
    "将非线性特性引入到神经网络中，否则输入和输出永远都是线性关系"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Thinking2:逻辑回归Logistic Loss是什么，解决分类问题 or 回归问题\n",
    "\n",
    "使用sigmoid函数来将具体的数值映射为0到1区间范围内，进行二分类。或者使用softmax函数进行多分类。\n",
    "解决分类问题"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
